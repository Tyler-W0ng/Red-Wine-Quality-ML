{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data = pd.read_csv('https://raw.githubusercontent.com/aniruddhachoudhury/Red-Wine-Quality/master/winequality-red.csv')\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_data.drop('quality', axis = 1)\n",
    "y = wine_data['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42, shuffle = True)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "y_train = y_train\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, in_features = 11, h1_output = 8, h2_output = 4, out_features = 10):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(in_features, h1_output)\n",
    "        self.h2 = nn.Linear(h1_output, h2_output)\n",
    "        self.out = nn.Linear(h2_output, out_features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.h1(X))\n",
    "        X = F.relu(self.h2(X))\n",
    "        X = F.softmax(self.out(X), dim = 1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 2.348125696182251\n",
      "Epoch: 10, loss: 2.2917656898498535\n",
      "Epoch: 20, loss: 2.2506840229034424\n",
      "Epoch: 30, loss: 2.1689884662628174\n",
      "Epoch: 40, loss: 2.031416416168213\n",
      "Epoch: 50, loss: 2.0167880058288574\n",
      "Epoch: 60, loss: 1.970512866973877\n",
      "Epoch: 70, loss: 1.9561073780059814\n",
      "Epoch: 80, loss: 1.949344515800476\n",
      "Epoch: 90, loss: 1.9452476501464844\n",
      "Epoch: 100, loss: 1.941028356552124\n",
      "Epoch: 110, loss: 1.9356005191802979\n",
      "Epoch: 120, loss: 1.9308308362960815\n",
      "Epoch: 130, loss: 1.9256612062454224\n",
      "Epoch: 140, loss: 1.9206095933914185\n",
      "Epoch: 150, loss: 1.9155254364013672\n",
      "Epoch: 160, loss: 1.9097431898117065\n",
      "Epoch: 170, loss: 1.8990960121154785\n",
      "Epoch: 180, loss: 1.8975235223770142\n",
      "Epoch: 190, loss: 1.8875007629394531\n",
      "Epoch: 200, loss: 1.885252594947815\n",
      "Epoch: 210, loss: 1.8816171884536743\n",
      "Epoch: 220, loss: 1.8789758682250977\n",
      "Epoch: 230, loss: 1.8768796920776367\n",
      "Epoch: 240, loss: 1.875440001487732\n",
      "Epoch: 250, loss: 1.8738787174224854\n",
      "Epoch: 260, loss: 1.8723937273025513\n",
      "Epoch: 270, loss: 1.8714267015457153\n",
      "Epoch: 280, loss: 1.8706028461456299\n",
      "Epoch: 290, loss: 1.8694286346435547\n",
      "Epoch: 300, loss: 1.8686202764511108\n",
      "Epoch: 310, loss: 1.8673624992370605\n",
      "Epoch: 320, loss: 1.8666306734085083\n",
      "Epoch: 330, loss: 1.8661991357803345\n",
      "Epoch: 340, loss: 1.866165280342102\n",
      "Epoch: 350, loss: 1.865533709526062\n",
      "Epoch: 360, loss: 1.864771842956543\n",
      "Epoch: 370, loss: 1.8644394874572754\n",
      "Epoch: 380, loss: 1.8644765615463257\n",
      "Epoch: 390, loss: 1.8641996383666992\n",
      "Epoch: 400, loss: 1.8637765645980835\n",
      "Epoch: 410, loss: 1.8632475137710571\n",
      "Epoch: 420, loss: 1.8628865480422974\n",
      "Epoch: 430, loss: 1.862723469734192\n",
      "Epoch: 440, loss: 1.8627396821975708\n",
      "Epoch: 450, loss: 1.862857699394226\n",
      "Epoch: 460, loss: 1.8624961376190186\n",
      "Epoch: 470, loss: 1.8623920679092407\n",
      "Epoch: 480, loss: 1.8619349002838135\n",
      "Epoch: 490, loss: 1.8616902828216553\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if(epoch%10 == 0):\n",
    "        print(f'Epoch: {epoch}, loss: {loss.detach().numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(3)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(7)\n",
      "tensor(6) tensor(8)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(4)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(4)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(8)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(4)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(8)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(8)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(7)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(4)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(4)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(4)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(4)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(7)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(4)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(4)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(8)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(5)\n",
      "tensor(6) tensor(7)\n",
      "tensor(6) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(6)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(6) tensor(6)\n",
      "tensor(5) tensor(4)\n",
      "Accuracy: 178/320\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    y_test_pred = model.forward(X_test)\n",
    "    for i in range(y_test_pred.shape[0]):\n",
    "        if(y_test_pred[i].argmax() == y_test[i]):\n",
    "            correct += 1\n",
    "        print(y_test_pred[i].argmax(), y_test[i])\n",
    "\n",
    "print(f'Accuracy: {correct}/{y_test_pred.shape[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
